import MDAnalysis as mda
import numpy as np
from scipy.spatial import distance_matrix
from tqdm import tqdm
from joblib import Parallel, delayed
import logging
import os

# Initialize logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger()

# Parameters
pdb_name = 'example/example.pdb'
cutoff_cg = 6.5 # Cutoff of CafeMol default parameter
cutoff_aa = 6.0 # Cutoff of GROMACS default parameter
nt = 10 # # Number of parallel threads
para_factor = 0.2 # Scale factor for final output

# Extract contact information from em1.ninfo
if not os.path.exists("output"):
    os.makedirs("output")
with open("example/em1.ninfo", "r") as infile, open("output/contact.txt", "w") as outfile:
    lines_written = sum(1 for line in infile if line.startswith("contact") and outfile.write(line))
logger.info(f"Extracted {lines_written} contact lines from em1.ninfo.")

# Load PDB file
u = mda.Universe(pdb_name).select_atoms("protein and not type H")
u.atoms.write('output/protein.pdb')

u = mda.Universe('output/protein.pdb')
chainids = np.unique(u.atoms.chainIDs)
logger.info(f"Chains identified: {chainids}")

# Compute offsets for residue indices across chains
offset_list = np.cumsum([0] + [u.select_atoms(f"chainID {c}").residues.n_residues for c in chainids])
logger.info(f"Computed offsets: {offset_list}")

# Generate contact list
contact_list = []
for i, ci in enumerate(chainids):
    agi = u.select_atoms(f'chainID {ci} and name CA')
    for resi in range(agi.n_residues):
        for resj in range(resi + 4, agi.n_residues):  # Skip 3 residues
            if distance_matrix(agi.residues[resi].atoms.positions, agi.residues[resj].atoms.positions).min() < cutoff_cg:
                contact_list.append([resi + offset_list[i], resj + offset_list[i]])
    for j, cj in enumerate(chainids[i + 1:], start=i + 1):
        agj = u.select_atoms(f'chainID {cj} and name CA')
        for resi in range(agi.n_residues):
            for resj in range(agj.n_residues):
                if distance_matrix(agi.residues[resi].atoms.positions, agj.residues[resj].atoms.positions).min() < cutoff_cg:
                    contact_list.append([resi + offset_list[i], resj + offset_list[j]])
logger.info(f"Generated {len(contact_list)} contact pairs.")

# Load old contact information
old = np.loadtxt('output/contact.txt', dtype=str)

# Validate contact numbers
if len(old) != len(contact_list):
    logger.error(f"em1.ninfo contacts nums {len(old)} != contacts nums {len(contact_list)} generated by this script")
    exit(1)
else:
    logger.info(f"em1.ninfo contacts nums {len(old)} match contacts nums {len(contact_list)} generated by this script")

# Load trajectory
u = mda.Universe('example/ref.pdb', 'example/fit.xtc')
n_frames = u.trajectory.n_frames
logger.info(f"Trajectory loaded with {n_frames} frames.")

# Compute contact counts per frame
def compute_contact_for_frame(frame_idx):
    u.trajectory[frame_idx]
    frame_contact_counts = []
    for resi, resj in contact_list:
        agi = u.residues[resi].atoms.select_atoms('not type H')
        agj = u.residues[resj].atoms.select_atoms('not type H')
        frame_contact_counts.append(np.sum(distance_matrix(agi.positions, agj.positions) < cutoff_aa))
    return frame_contact_counts

logger.info("Computing contact counts for each frame...")
contact_count = np.array(Parallel(n_jobs=nt)(delayed(compute_contact_for_frame)(i) for i in tqdm(range(n_frames))))
np.save('output/contact_count.npy', contact_count)
logger.info(f"Saved contact counts to 'contact_count.npy'.")

# Compute average, std deviation, and scale
ave = np.mean(contact_count, axis=0)
std = np.std(contact_count, axis=0)
scale = np.divide(ave, std, out=np.zeros_like(ave), where=std != 0)
ave_std_scale = np.vstack((ave, std, scale)).T

# Save normalized contacts
new_contact_data = np.hstack((np.array(contact_list, dtype=int) + 1, ave_std_scale))
np.savetxt('output/new_contact.txt', new_contact_data, fmt='%d %d %.6f %.6f %.6f')
logger.info(f"Saved normalized contact data to 'new_contact.txt'.")

# Update old contact data with new scale factors
for i in range(len(old)):
    if int(old[i, 4]) == int(new_contact_data[i, 0]) and int(old[i, 5]) == int(new_contact_data[i, 1]):
        old_factor_go = float(old[i, 9])
        old_coef_go = float(old[i, 11])
        old[i, 11] = old_coef_go / old_factor_go * float(new_contact_data[i, 4])
        old[i, 9] = new_contact_data[i, 4]
    else:
        logger.warning(f"Old contact {old[i, 4]}-{old[i, 5]} does not match {int(new_contact_data[i, 0])}-{int(new_contact_data[i, 1])}")

# Contact statistics
contact_counts = {}
for row in old:
    key = (row[2], row[3])
    contact_counts[key] = contact_counts.get(key, 0) + 1

# Write final contact data
with open('output/final_contact.txt', 'w') as output_file:
    last_key = None
    for row in old:
        key = (row[2], row[3])
        if key != last_key:
            total_contact = contact_counts[key]
            output_file.write(f"\n** contact between unit {key[0]:>6} and {key[1]:>6}\n")
            output_file.write(f"** total_contact_unit = {total_contact:>6}\n")
            output_file.write("**        icon iunit1-iunit2   imp1 - imp2 imp1un-imp2un      go_nat     factor_go  dummy     coef_go\n")
            last_key = key
        formatted_row = f"{row[0]:<7} {row[1]:>6} {row[2]:>6} {row[3]:>6} {row[4]:>6} " \
                        f"{row[5]:>6} {row[6]:>6} {row[7]:>6} " \
                        f"{float(row[8]):>12.4f} {float(row[9]) * para_factor:>12.4f} {row[10]:>6} {float(row[11]) * para_factor:>12.4f} {row[12]:>3}"
        output_file.write(formatted_row + '\n')
logger.info(f"Final contact data written to 'final_contact.txt'.")

